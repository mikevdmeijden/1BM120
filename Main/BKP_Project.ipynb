{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhI1eNEXQCjK"
   },
   "source": [
    "# KnapsackV2\n",
    "1BM120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkMMK4kj5h74"
   },
   "source": [
    "## Part 0: Load libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxxpHDIs_lvg",
    "outputId": "661c0206-3050-4de4-d370-380bddd88eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: or_gym in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from or_gym) (1.5.2)\n",
      "Requirement already satisfied: gym>=0.15.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from or_gym) (0.18.0)\n",
      "Requirement already satisfied: networkx>=2.3 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from or_gym) (2.5)\n",
      "Requirement already satisfied: numpy>=1.16.1 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from or_gym) (1.19.2)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from or_gym) (3.3.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym>=0.15.0->or_gym) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym>=0.15.0->or_gym) (7.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from gym>=0.15.0->or_gym) (1.6.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from networkx>=2.3->or_gym) (4.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from matplotlib>=3.1->or_gym) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from matplotlib>=3.1->or_gym) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from matplotlib>=3.1->or_gym) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from matplotlib>=3.1->or_gym) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from matplotlib>=3.1->or_gym) (2.4.7)\n",
      "Requirement already satisfied: future in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.15.0->or_gym) (0.18.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s153988\\documents\\anaconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib>=3.1->or_gym) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install or_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9qH33L_QoBk"
   },
   "source": [
    "    Import the neccary packages\n",
    "- `Numpy` for our Qtable\n",
    "- `OpenAI Gym` for our FrozenLake Environment\n",
    "- `Random` to generate random numbers\n",
    "- `Deque` to record cumulative reward over multiple episodes\n",
    "- `MatPlotlib` to generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oU8zRXv8QHlm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import or_gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fz-X3HTQueX"
   },
   "source": [
    "## Part 1: Explore Knapsack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Task 1:** \n",
    "- Create an instance of the environment\n",
    "- Render the environment \n",
    "- Print the state and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mh9jBR_cQ5_a"
   },
   "outputs": [],
   "source": [
    "env = or_gym.make(\"Knapsack-v2\") # Create an  instance of the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8PdZVucK_jMf",
    "outputId": "c1aff157-3aa7-46c8-f368-0cc5de2af402"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-eb1347a3c459>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# render the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Anaconda\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    108\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# just raise an exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.render()# render the environment \n",
    "env.mask = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVpKDNoZ7qQ1"
   },
   "source": [
    "##### The agent moves through a $X\\times X$ gridworld\n",
    "\n",
    "The agent has X potential actions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1odpibX7rdQ",
    "outputId": "fcd38795-2783-4ffa-be2f-20103118e973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space 4\n",
      "State space 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state_space = env.reset()#env.observation_space.n#Get number of states in an environment \n",
    "action_space = env.action_space.n#env.action_space.n#Get number of actions in an environment \n",
    "print('Action space', action_space)\n",
    "print('State space', state_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-QK-SPd8C5h"
   },
   "source": [
    "Other important points of the environvment:\n",
    "- The episode ends when you reach the goal or fall in a hole. \n",
    "- Agent receives a reward of 1 if it reach the goal, and zero otherwise.\n",
    "- FrozenLake-v0 is considered \"solved\" when the agent obtains an average reward of at least **0.78 over 100** consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "- Sample an action from the environment.\n",
    "- Call the step function and inspect the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()# sample an action from the env instance\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.0, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)# call the step function of the env. and inspect quadruple of (state, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dQ6sZJR-39K"
   },
   "source": [
    "**Task 3:**\n",
    "- Randomly interact with the envrionment for two episodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WkS_hnft-3A2",
    "outputId": "448fa484-b06b-43a6-c4e5-650e0bef88ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE  0\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 1\n",
      "EPISODE  1\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "We fell into a hole ‚ò†Ô∏è\n",
      "Number of steps 9\n"
     ]
    }
   ],
   "source": [
    "for episode in range(2):\n",
    "    state = env.reset()# get the starting state from the env.\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(99): \n",
    "        action = env.action_space.sample()# sample an action from the environment  \n",
    "        new_state, reward, done, info = env.step(action) #give the action to environment to obtain reward, and next state,   \n",
    "        if done: #if the goal state is reached or agent fall into hole. \n",
    "            env.render() #print the last stay\n",
    "            if new_state == 15:\n",
    "                print(\"We reached our Goal üèÜ\")\n",
    "            else:\n",
    "                print(\"We fell into a hole ‚ò†Ô∏è\")\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            \n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEtXMldxQ7uw"
   },
   "source": [
    "## Part 2: Q-Table üóÑÔ∏è\n",
    "We will implement Q-learning algorithm to devise optimal policy for FrozenLake environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "- Create Q-table with `state space` as rows and `action space` as columns.\n",
    "state =16  action=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "noM2G-NsF6Z9",
    "outputId": "c181f6f0-b3da-45af-aad8-de9ab638618b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_space, action_space))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcdaN_DbA3ES"
   },
   "source": [
    "## Part 3: The Q learning algorithm üß†\n",
    "It is fine if you do not understand all the details at this point. Q-learning will be introduced in Lecture 3 of DRL part. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh_0vS_dBPCY"
   },
   "source": [
    "Q learning is a off-policy algorithm. Meaning that the actions that are executed are different from the target actions that are used for learning. \n",
    "Epsilon-greedy policy ‚Äì most likely selects the `greedy actions` but can select `random actions` too \n",
    "- Ensures explorations\n",
    "- Choose greedy action with 1- —î (epsilon) \n",
    "- Choose random action with —î (epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "iBiRANB2AH3c"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon): \n",
    "      # Q:          : state-action pair\n",
    "      # State (int) : current state\n",
    "      # eps (float): epsilon\n",
    "    action = 0\n",
    "    if random.uniform(0, 1) > epsilon: #exploitation\n",
    "        action = np.argmax(Q[state,:])\n",
    "    else: #exploration \n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOB4TejJM4uA"
   },
   "source": [
    "\n",
    "The algorithm takes nine arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `total_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `max_step`: This is the max number of interactions between agent and env. within a single episode.  \n",
    "- `epsilon`: This is to encourage exploration. Epsilon is decayed over time to discourage explortation and encourage exploitation once agent has explored different state. \n",
    "- `max_epsilon`: This is the maximum value of epsilon. \n",
    "- `min_epsilon`: This is the minimum value of epsilon. \n",
    "- `decay_rate`: This is the decay rate for epsilon. \n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `plot_every`: This is additional argument to plot the cumulative reward against episodes. \n",
    "\n",
    "The algorithm returns as output:\n",
    "- `qtable`: This is an ndarray where `qtable[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5**\n",
    "- Fill the missing code to complete the Q-learning implementation.\n",
    "- Write a condition to break the loop as soon as agent receives a reward of 0.78 or higher in 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3cNXlxF6F4k1"
   },
   "outputs": [],
   "source": [
    "def q_learning(env, total_episodes, max_steps = 99, epsilon = 1.0, max_epsilon = 1.0, min_epsilon = 0.01, decay_rate = 0.005,  gamma=0.95, plot_every=100):\n",
    "    rewards = []   # List of rewards\n",
    "    tmp_scores = deque(maxlen=plot_every)     # deque for keeping track of scores\n",
    "    avg_scores = deque(maxlen=total_episodes)   # average scores over every plot_every episodes\n",
    "    for episode in range(total_episodes):\n",
    "        state = env.reset()#Reset the environment to the starting state \n",
    "        #step = 0 \n",
    "        done = False\n",
    "        total_rewards = 0 # collected reward within an episode\n",
    "        if episode % 100 == 0: #monitor progress\n",
    "            print(\"\\rEpisode {}/{}\".format(episode, total_episodes), end=\"\") \n",
    "        \n",
    "        for step in range(max_steps): \n",
    "            action = epsilon_greedy_policy(qtable, state, epsilon)# call the epsilon greedy policy to obtain the actions  \n",
    "            new_state, reward, done, info = env.step(action) #take the action and observe resulting reward and state. \n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            # qtable[new_state,:] : all the actions we can take from new state\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action]) #update the qtable. np.max(qtable[new_state, :] is greedy action used for learning!. \n",
    "\n",
    "            total_rewards += reward # sum the rewards collected within an episode\n",
    "            state = new_state # Our new state is state\n",
    "            if done == True: #done is true when agent fall into hole or reached the goal state\n",
    "                tmp_scores.append(total_rewards)  #for plot\n",
    "                break\n",
    "        if (episode % plot_every == 0): #for plot\n",
    "            avg_scores.append(np.mean(tmp_scores))\n",
    "            \n",
    "            #....  #break the loop as soon as agent obtain the reward of 0.78 or higher in 100 consective episodes. \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) # Reduce epsilon value to encourage expoitation and discouage exlortation \n",
    "        rewards.append(total_rewards)\n",
    "\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,total_episodes,len(avg_scores),endpoint=False), np.asarray(avg_scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(avg_scores))    \n",
    "    return qtable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEGeWKKsAu7X"
   },
   "source": [
    "## Part 4: Train the agent  ü§ñ\n",
    "Here comes the real part. \n",
    "- We will train our agent using Q-learning algorithm defined above.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6**\n",
    "- Call the Q-learning algorithm with appropriate hyperparameter setting. \n",
    "- Find the hyper-parameters configuration  to solve the environment in fewer than 5000 training episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FJhPxx7UAunE"
   },
   "outputs": [],
   "source": [
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.2#7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "lGABCf7TGHRH",
    "outputId": "98e11365-ca70-4d52-c189-f13e0b466829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19900/20000"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Average Reward over 100 Episodes:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning(env, total_episodes, epsilon = 1.0, gamma=0.95, plot_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "R5czk9qTBQIU"
   },
   "source": [
    "## Part 5: Action in Action! üïπÔ∏è\n",
    "- After training, the agent has develop a Q-table can be used to play FrozenLake. The Q-table tells agent which action to take in each state. \n",
    "- Run the code below to see our agent playing FrozenLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "Bt8UsREaBNkJ",
    "outputId": "8aa495fa-08e8-4044-e143-6d7c681a9817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "EPISODE  0\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Hole üíÄ\n",
      "Number of steps 15\n",
      "********************\n",
      "EPISODE  1\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Hole üíÄ\n",
      "Number of steps 10\n",
      "********************\n",
      "EPISODE  2\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Hole üíÄ\n",
      "Number of steps 38\n",
      "********************\n",
      "EPISODE  3\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Hole üíÄ\n",
      "Number of steps 7\n",
      "********************\n",
      "EPISODE  4\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Hole üíÄ\n",
      "Number of steps 8\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"********************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(qtable[state,:])# Take the action (index) with maximum expected future reward given that state\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            if new_state == 15:\n",
    "                print(\"Goal üèÖ\")\n",
    "            else:\n",
    "                print(\"Hole üíÄ\")            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)            \n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RkMMK4kj5h74",
    "0fz-X3HTQueX",
    "JEtXMldxQ7uw",
    "rcdaN_DbA3ES",
    "WEGeWKKsAu7X",
    "R5czk9qTBQIU"
   ],
   "name": "Solving FrozenLake using Q-learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
